{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import PCA, StopWordsRemover, Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors,VectorUDT\n",
    "import datetime\n",
    "start_time=datetime.datetime.now()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Assignment2 - COMP5349-Stage3\") \\\n",
    "    .getOrCreate()\n",
    "awsData = \"s3://amazon-reviews-pds/tsv/amazon_reviews_us_Music_v1_00.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: string (nullable = true)\n",
      " |-- helpful_votes: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading data into a dataframe\"\"\"\n",
    "musicData = spark.read.csv(awsData,header=True,sep='\\t')\n",
    "musicData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4751577"
     ]
    }
   ],
   "source": [
    "\"\"\"Selecting required columns\"\"\"\n",
    "requiredData = musicData.select('customer_id','product_id','product_title','star_rating','review_id','review_body')\n",
    "requiredData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Identifying top 10 products based on number of reviews received.\"\"\"\n",
    "top_10_product_ids=requiredData.filter(col(\"review_id\").isNotNull()).groupBy(\"product_id\").count().sort(col(\"count\").desc()).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Picking 10th product from the top 10 products identified\"\"\"\n",
    "picked_product=top_10_product_ids.sort(col(\"count\")).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Separating positive and negative reviews based on star rating\"\"\"\n",
    "positive_reviews=requiredData.join(picked_product.select(\"product_id\"),\"product_id\",\"inner\").filter(col(\"star_rating\")>=4)\n",
    "negative_reviews=requiredData.join(picked_product.select(\"product_id\"),\"product_id\",\"inner\").filter(col(\"star_rating\")<=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323"
     ]
    }
   ],
   "source": [
    "positive_reviews.cache()\n",
    "positive_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415"
     ]
    }
   ],
   "source": [
    "negative_reviews.cache()\n",
    "negative_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting review body into sentences based on . or ? and saving as list\"\"\"\n",
    "positive_review_sentences=positive_reviews.filter(col(\"review_body\").isNotNull()).select(\"review_id\",\"review_body\")\n",
    "positive_review_sentences=positive_review_sentences.withColumn(\"review_body_sentences\",split(col(\"review_body\"), r\"\\.|\\?\"))\n",
    "#positive_review_sentences.show(2,truncate=False)\n",
    "negative_review_sentences=negative_reviews.filter(col(\"review_body\").isNotNull()).select(\"review_id\",\"review_body\")\n",
    "negative_review_sentences=negative_review_sentences.withColumn(\"review_body_sentences\",split(col(\"review_body\"), r\"\\.|\\?\"))\n",
    "#negative_review_sentences.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Exploding review sentences to have one to many mapping for review_id vs review sentences. Also, cleaning review sentences \n",
    "of any special characters/trim additional spaces. Later on, we split sentence into words to remove any stop words in next step\"\"\"\n",
    "positive_review_sentences=positive_review_sentences.select(\"review_id\",\"review_body_sentences\",\"review_body\")\n",
    "positive_review_sentences=positive_review_sentences.withColumn(\"review_body_sentences\",explode(\"review_body_sentences\"))\n",
    "positive_review_sentences=positive_review_sentences.withColumn(\"review_body_sentences_joined\",col(\"review_body_sentences\"))\n",
    "positive_review_sentences=positive_review_sentences.withColumn(\"review_body_sentences\",regexp_replace(\"review_body_sentences\",\"<br />|\\s+|$|,|!|#|@|<|>|/|&|#|;|:|[0-9]|-\",\" \")) \\\n",
    ".withColumn(\"review_body_sentences\",regexp_replace(\"review_body_sentences\",\"\\s+\",\" \")) \\\n",
    ".withColumn(\"review_body_sentences\",trim(col(\"review_body_sentences\"))) \\\n",
    ".filter(col(\"review_body_sentences\").isNotNull()) \\\n",
    ".filter(col(\"review_body_sentences\")!='') \\\n",
    ".withColumn(\"review_body_sentences\",lower(col(\"review_body_sentences\"))) \\\n",
    ".withColumn(\"review_body_sentences\",split(trim(col(\"review_body_sentences\")), \"\\s+\"))\n",
    "#positive_review_sentences.show(2,truncate=False)\n",
    "\n",
    "negative_review_sentences=negative_review_sentences.select(\"review_id\",\"review_body_sentences\",\"review_body\")\n",
    "negative_review_sentences=negative_review_sentences.withColumn(\"review_body_sentences\",explode(\"review_body_sentences\"))\n",
    "negative_review_sentences=negative_review_sentences.withColumn(\"review_body_sentences_joined\",col(\"review_body_sentences\"))\n",
    "negative_review_sentences=negative_review_sentences.withColumn(\"review_body_sentences\",regexp_replace(\"review_body_sentences\",\"<br />|\\s+|$|,|!|#|@|<|>|/|&|#|;|:|[0-9]|-\",\" \")) \\\n",
    ".withColumn(\"review_body_sentences\",regexp_replace(\"review_body_sentences\",\"\\s+\",\" \")) \\\n",
    ".withColumn(\"review_body_sentences\",trim(col(\"review_body_sentences\"))) \\\n",
    ".filter(col(\"review_body_sentences\").isNotNull()) \\\n",
    ".filter(col(\"review_body_sentences\")!='') \\\n",
    ".withColumn(\"review_body_sentences\",lower(col(\"review_body_sentences\"))) \\\n",
    ".withColumn(\"review_body_sentences\",split(trim(col(\"review_body_sentences\")), \"\\s+\"))\n",
    "#positive_review_sentences.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stop words remover will remove stop words. Stop words list is retreived from nltk using below:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " set(stopwords.words('english'))\n",
    "\"\"\"\n",
    "stopwords_list=['a', 'ah','br',  'about', 'quot', 'above',  'after',  'again',  'against',  'ain',  'all',  'am',  'an',  'and',  'any',  'are',  'aren',  \"aren't\",  'as',  'at',  'be',  'because',  'been',  'before',  'being',  'below',  'between',  'both',  'but',  'by',  'can',  'couldn',  \"couldn't\",  'd',  'did',  'didn',  \"didn't\",  'do',  'does',  'doesn',  \"doesn't\",  'doing',  'don',  \"don't\",  'down',  'during',  'each',  'few',  'for',  'from',  'further',  'had',  'hadn',  \"hadn't\",  'has',  'hasn',  \"hasn't\",  'have',  'haven',  \"haven't\",  'having',  'he',  'her',  'here',  'hers',  'herself',  'him',  'himself',  'his',  'how',  'i',  'if',  'in',  'into',  'is',  'isn',  \"isn't\",  'it',  \"it's\",  'its',  'itself',  'just',  'll',  'm',  'ma',  'me',  'mightn',  \"mightn't\",  'more',  'most',  'mustn',  \"mustn't\",  'my',  'myself',  'needn',  \"needn't\",  'no',  'nor',  'not',  'now',  'o',  'of',  'off',  'on',  'once',  'only',  'or',  'other',  'our',  'ours',  'ourselves',  'out',  'over',  'own',  're',  's',  'same',  'shan',  \"shan't\",  'she',  \"she's\",  'should',  \"should've\",  'shouldn',  \"shouldn't\",  'so',  'some',  'such',  't',  'than',  'that',  \"that'll\",  'the',  'their',  'theirs',  'them',  'themselves',  'then',  'there',  'these',  'thanks',  'they',  'this',  'those',  'through',  'to',  'too',  'under',  'until',  'up',  've',  'very',  'was',  'wasn',  \"wasn't\",  'we',  'were',  'weren',  \"weren't\",  'what',  'when',  'where',  'which',  'while',  'who',  'whom',  'why',  'will',  'with',  'won',  \"won't\",  'wouldn',  \"wouldn't\",  'y',  'you',  \"you'd\",  \"you'll\",  \"you're\",  \"you've\",  'your',  'yours',  'yourself',  'yourselves']\n",
    "removerSW=StopWordsRemover(inputCol=\"review_body_sentences\",outputCol=\"filtered\", stopWords=stopwords_list)\n",
    "positive_review_sentences_new=removerSW.transform(positive_review_sentences)\n",
    "negative_review_sentences_new=removerSW.transform(negative_review_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8667"
     ]
    }
   ],
   "source": [
    "\"\"\"removing any sentences with less than two words in them\"\"\"\n",
    "positive_review_sentences_new=positive_review_sentences_new.filter(size(col(\"filtered\"))>2)\n",
    "positive_review_sentences_new.cache()\n",
    "positive_review_sentences_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3048"
     ]
    }
   ],
   "source": [
    "negative_review_sentences_new=negative_review_sentences_new.filter(size(col(\"filtered\"))>2)\n",
    "negative_review_sentences_new.cache()\n",
    "negative_review_sentences_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Joining back the words splitted into sentences back as tensorflow hub works on sentences as well\"\"\"\n",
    "def join_strings(list_of_strings):\n",
    "    string=\" \".join(list_of_strings)\n",
    "    return string\n",
    "    \n",
    "join_strings_udf=udf(join_strings,StringType())\n",
    "positive_review_sentences_new=positive_review_sentences_new.withColumn(\"filtered_joined\",join_strings_udf(col(\"filtered\")))\n",
    "negative_review_sentences_new=negative_review_sentences_new.withColumn(\"filtered_joined\",join_strings_udf(col(\"filtered\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adding index column to the dataframe\"\"\"\n",
    "positive_review_sentences_main=positive_review_sentences_new.rdd.zipWithUniqueId().map(lambda x: (x[1],x[0][\"review_id\"],x[0][\"review_body_sentences\"],x[0][\"review_body_sentences_joined\"],x[0][\"filtered\"],x[0][\"filtered_joined\"])).toDF()\n",
    "negative_review_sentences_main=negative_review_sentences_new.rdd.zipWithUniqueId().map(lambda x: (x[1],x[0][\"review_id\"],x[0][\"review_body_sentences\"],x[0][\"review_body_sentences_joined\"],x[0][\"filtered\"],x[0][\"filtered_joined\"])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Forming main dataframe and renaming columns\"\"\"\n",
    "positive_review_sentences_main=positive_review_sentences_main.select(col(\"_1\").alias(\"index\"),col(\"_2\").alias(\"review_id\"),col(\"_3\").alias(\"review_body_sentences\"),col(\"_4\").alias(\"review_body_sentences_joined\"),col(\"_5\").alias(\"filtered\"),col(\"_6\").alias(\"filtered_joined\"))\n",
    "negative_review_sentences_main=negative_review_sentences_main.select(col(\"_1\").alias(\"index\"),col(\"_2\").alias(\"review_id\"),col(\"_3\").alias(\"review_body_sentences\"),col(\"_4\").alias(\"review_body_sentences_joined\"),col(\"_5\").alias(\"filtered\"),col(\"_6\").alias(\"filtered_joined\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defining function to work on partition to convert sentences into vectors.\"\"\"\n",
    "def review_embed(rev_text_partition):\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "    embed = hub.Module(module_url)\n",
    "    rev_text_list = [text for text in rev_text_partition]\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(rev_text_list))\n",
    "    return message_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "\"\"\"Extracting only review sentences field and converting into string. We already took sentences with more than 2 words, \n",
    "so there wont be any nulls. Ensuring we have only 1 partition\"\"\"\n",
    "positive_review_sentences_rdd=positive_review_sentences_main.select(\"filtered_joined\").rdd.map(lambda x: str(x[0])).cache()  #.repartition(1000)\n",
    "positive_review_sentences_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "negative_review_sentences_rdd=negative_review_sentences_main.select(\"filtered_joined\").rdd.map(lambda x: str(x[0])).cache()  #.repartition(1000)\n",
    "negative_review_sentences_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Applying function on the rdd formed\"\"\"\n",
    "positive_review_embed=positive_review_sentences_rdd.mapPartitions(review_embed).cache()\n",
    "negative_review_embed=negative_review_sentences_rdd.mapPartitions(review_embed).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review_embed_vectors_df=positive_review_embed.zipWithUniqueId().map(lambda x: (x[1],Vectors.dense(x[0].tolist()))).toDF().select(col(\"_1\").alias(\"index\"),col(\"_2\").alias(\"features\"))\n",
    "negative_review_embed_vectors_df=negative_review_embed.zipWithUniqueId().map(lambda x: (x[1],Vectors.dense(x[0].tolist()))).toDF().select(col(\"_1\").alias(\"index\"),col(\"_2\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementing PCA to convert into 2 dimensional vector\"\"\"\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(positive_review_embed_vectors_df)\n",
    "pca_result_stg = model.transform(positive_review_embed_vectors_df)\n",
    "model_n = pca.fit(negative_review_embed_vectors_df)\n",
    "pca_result_stg_n = model_n.transform(negative_review_embed_vectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setting up normaliser to allow taking dot product for cosine similarity\"\"\"\n",
    "normalizer = Normalizer(inputCol=\"pca_features\", outputCol=\"features_norm\", p=2.0)\n",
    "pca_result_stg = normalizer.transform(pca_result_stg)\n",
    "pca_result_stg_n = normalizer.transform(pca_result_stg_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8667, 2)"
     ]
    }
   ],
   "source": [
    "\"\"\"Collecting all positive normalised vectors into numpy array and reshaping to have each row to contain \n",
    "vector from corresponding review sentence \"\"\"\n",
    "all_vectors_array=np.array(pca_result_stg.select(\"features_norm\").collect())\n",
    "all_vectors_array=all_vectors_array.reshape((all_vectors_array.shape[0],2))\n",
    "all_vectors_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3048, 2)"
     ]
    }
   ],
   "source": [
    "\"\"\"Similarly for negative\"\"\"\n",
    "all_vectors_array_n=np.array(pca_result_stg_n.select(\"features_norm\").collect())\n",
    "all_vectors_array_n=all_vectors_array_n.reshape((all_vectors_array_n.shape[0],2))\n",
    "all_vectors_array_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Making join with pca vectors with main dataframe based on index to get relevant fields required\"\"\"\n",
    "pca_result=positive_review_sentences_main.join(pca_result_stg,\"index\",\"inner\")\n",
    "pca_result_n=negative_review_sentences_main.join(pca_result_stg_n,\"index\",\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defining function to take intra class similarity. It takes dot product between normalised features with the array formed \n",
    "in previous steps. We exclude the vector itself from the main array as we aim to find similarity between other vectors. This \n",
    "is achieved by droping the column at index of the row feature as took transpose for dot product.Once dot product is done, \n",
    "we take 1-dot product to get cosine distance and take average using numpy.mean\"\"\"\n",
    "def intra_class_similarity(features_vec,index):\n",
    "    current_vec=np.array(features_vec)[np.newaxis,:]\n",
    "    all_vec_norm=all_vectors_array.T\n",
    "    all_vec_norm=np.delete(all_vec_norm, index, axis=1)\n",
    "    avg_dist=np.mean(1-np.dot(current_vec,all_vec_norm))\n",
    "    return float(avg_dist)\n",
    "\n",
    "def intra_class_similarity_n(features_vec,index):\n",
    "    current_vec=np.array(features_vec)[np.newaxis,:]\n",
    "    all_vec_norm=all_vectors_array_n.T\n",
    "    all_vec_norm=np.delete(all_vec_norm, index, axis=1)\n",
    "    avg_dist=np.mean(1-np.dot(current_vec,all_vec_norm))\n",
    "    return float(avg_dist)\n",
    "\n",
    "\n",
    "intra_class_similarity_udf = udf(intra_class_similarity, FloatType())\n",
    "intra_class_similarity_n_udf = udf(intra_class_similarity_n, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Applying UDF defined in previous step for both negative and positive cases\"\"\"\n",
    "pca_result_withIndex=pca_result.withColumn(\"avg_distance\", intra_class_similarity_udf(pca_result[\"features_norm\"],pca_result[\"index\"]))\n",
    "pca_result_withIndex_n=pca_result_n.withColumn(\"avg_distance\", intra_class_similarity_n_udf(pca_result_n[\"features_norm\"],pca_result_n[\"index\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_upd=pca_result_withIndex.select(\"index\",\"review_id\",\"pca_features\",\"review_body_sentences_joined\",\"filtered\",\"features_norm\",\"avg_distance\").sort(col(\"avg_distance\"))\n",
    "pca_result_upd_n=pca_result_withIndex_n.select(\"index\",\"review_id\",\"pca_features\",\"review_body_sentences_joined\",\"filtered\",\"features_norm\",\"avg_distance\").sort(col(\"avg_distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_result_upd.agg({\"avg_distance\": \"max\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_center=pca_result_upd.limit(1)\n",
    "\"\"\"Extracting case center by sorting in ascending based on average distance\"\"\"\n",
    "positive_center=pca_result_upd.select(\"index\",\"review_id\",\"features_norm\",\"avg_distance\",col(\"review_body_sentences_joined\").alias(\"center_sentence_text\")).limit(1)\n",
    "negative_center=pca_result_upd_n.select(\"index\",\"review_id\",\"features_norm\",\"avg_distance\",col(\"review_body_sentences_joined\").alias(\"center_sentence_text\")).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Using center identified, we apply the function similar to previous one, to identify distance and sort in ascending order. \n",
    "The first element is considered to be iself as vector will have 0 distance(least) with itself. \"\"\"\n",
    "def ten_most_similar(features_vec,index):\n",
    "    current_vec=np.array(features_vec)[np.newaxis,:]\n",
    "    all_vec_norm=all_vectors_array.T\n",
    "    #all_vec_norm=np.delete(all_vec_norm, index, axis=1)\n",
    "    index_list=np.argsort(1-np.dot(current_vec,all_vec_norm))[:,1:11][0]\n",
    "    return index_list.tolist()\n",
    "\n",
    "def ten_most_similar_n(features_vec,index):\n",
    "    current_vec=np.array(features_vec)[np.newaxis,:]\n",
    "    all_vec_norm=all_vectors_array_n.T\n",
    "    #all_vec_norm=np.delete(all_vec_norm, index, axis=1)\n",
    "    index_list=np.argsort(1-np.dot(current_vec,all_vec_norm))[:,1:11][0]\n",
    "    return index_list.tolist()\n",
    "\n",
    "ten_most_similar_udf = udf(ten_most_similar, ArrayType(IntegerType()))\n",
    "ten_most_similar_n_udf = udf(ten_most_similar_n, ArrayType(IntegerType()))\n",
    "\n",
    "ten_most_similar_df=positive_center.withColumn(\"10_most_similar_index\",ten_most_similar_udf(col(\"features_norm\"),col(\"index\")))\n",
    "ten_most_similar_df=ten_most_similar_df.drop(col(\"index\")).withColumn(\"index\",explode(\"10_most_similar_index\"))\n",
    "ten_most_similar_n_df=negative_center.withColumn(\"10_most_similar_index\",ten_most_similar_n_udf(col(\"features_norm\"),col(\"index\")))\n",
    "ten_most_similar_n_df=ten_most_similar_n_df.drop(col(\"index\")).withColumn(\"index\",explode(\"10_most_similar_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Based on index identified in previous step, we make a join with main dataframe and extract results in required format.\"\"\"\n",
    "final_result=ten_most_similar_df.select(col(\"review_id\").alias(\"center_review_id\"),col(\"avg_distance\").alias(\"center_avg_dist\"),col(\"index\"), col(\"center_sentence_text\")).join(pca_result_upd,\"index\", \"inner\")\n",
    "final_result_n=ten_most_similar_n_df.select(col(\"review_id\").alias(\"center_review_id\"),col(\"avg_distance\").alias(\"center_avg_dist\"),col(\"index\"), col(\"center_sentence_text\")).join(pca_result_upd_n,\"index\", \"inner\")\n",
    "#final_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_final=final_result.select(\"center_review_id\",\"center_sentence_text\",col(\"review_id\").alias(\"10_nearestneighbour_rv_id\"), col(\"review_body_sentences_joined\").alias(\"corresponding_sentence\"))\n",
    "negative_final=final_result_n.select(\"center_review_id\",\"center_sentence_text\",col(\"review_id\").alias(\"10_nearestneighbour_rv_id\"), col(\"review_body_sentences_joined\").alias(\"corresponding_sentence\"))\n",
    "#df.write.parquet(\"s3a://bucket-name/shri/test.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-------------------------+----------------------+\n",
      "|center_review_id|center_sentence_text|10_nearestneighbour_rv_id|corresponding_sentence|\n",
      "+----------------+--------------------+-------------------------+----------------------+\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R221HHEXXYR0O5|   Don't believe th...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R3Q0GV24AEX6EO|    These are two d...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R33LA8393ELKP9|  I love Good Charl...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R3L848KDDKFSGU|   to make it clear...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R1YX5THOWN73FR|   Geez Good Charlo...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R1QUKBXCN1EQSO|  It is so aweseome...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R1LAMC7N0Y96VI|   My bloody Valent...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R3UJ8LR0RK48OU|   The melodies are...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R1Y92IXSHHRWOM|  Just because a ba...|\n",
      "|   RLMNZTH4LDUAN|  My personal fav...|           R2K78UJNJE4RKO|   antiradioandmtv,...|\n",
      "+----------------+--------------------+-------------------------+----------------------+"
     ]
    }
   ],
   "source": [
    "\"\"\"Result for positive case. It includes center review id, center sentence text, \n",
    "10 nearest neighbours review id and their corresponding sentences\"\"\"\n",
    "positive_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-------------------------+----------------------+\n",
      "|center_review_id|center_sentence_text|10_nearestneighbour_rv_id|corresponding_sentence|\n",
      "+----------------+--------------------+-------------------------+----------------------+\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|           R246WGK1KFGIFO|   I mean the music...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|            RLKKDH4ZQTSR2|  i bought this alb...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|           R2Z6QVD6K0ILZ0|   You can save a t...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|            RRBM6IZ04NH41|   If one remark wo...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|           R377WFXFM7DCVY|   <br /> <br />Don...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|           R1O38JNU5QXPHT|    Good Charlotte ...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|           R3UX3SV2OQVRKX|   Terrible, just b...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|           R2WZV9V9DDU210|   This review is a...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|            RENINTS8VEFY1|   They are just th...|\n",
      "|  R3MGK1ZXH61TIS| now if you want ...|           R195DYX83KWYXU|   <br /> <br />  <...|\n",
      "+----------------+--------------------+-------------------------+----------------------+"
     ]
    }
   ],
   "source": [
    "\"\"\"Result for negative case. It includes center review id, center sentence text, \n",
    "10 nearest neighbours review id and their corresponding sentences\"\"\"\n",
    "negative_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time=datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0:02:35.047162"
     ]
    }
   ],
   "source": [
    "print(\"Execution time:\",(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+----------------------------------------+------------+------------------------------------------------+\n",
      "|index|review_id    |features_norm                           |avg_distance|center_sentence_text                            |\n",
      "+-----+-------------+----------------------------------------+------------+------------------------------------------------+\n",
      "|5152 |RLMNZTH4LDUAN|[0.9921261081421591,0.12524290615716627]|0.3372212   |  My personal favorites are tracks 1,2,3,4,and 8|\n",
      "+-----+-------------+----------------------------------------+------------+------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "positive_center.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+---------------------------------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|index|review_id     |features_norm                          |avg_distance|center_sentence_text                                                                                                                                                                                                                                                                                                     |\n",
      "+-----+--------------+---------------------------------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2175 |R3MGK1ZXH61TIS|[0.9973288588577638,0.0730420925868812]|0.38292772  | now if you want some real punk rock then i suggest going out and getting the Suicide Machines album Destruction by Definition but if ur gonna listen to this crap then u might as well pick up the new britney spears album along with it because my friend all your gettin is a fashion show with a suck-ass soundtrack|\n",
      "+-----+--------------+---------------------------------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "negative_center.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
