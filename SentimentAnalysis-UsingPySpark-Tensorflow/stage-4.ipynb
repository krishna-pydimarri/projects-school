{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1558512418166_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-42-31.us-east-2.compute.internal:20888/proxy/application_1558512418166_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-36-118.us-east-2.compute.internal:8042/node/containerlogs/container_1558512418166_0002_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.feature import PCA, StopWordsRemover, Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "start_time=datetime.datetime.now()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Assignment2 - COMP5349-Stage4\") \\\n",
    "    .getOrCreate()\n",
    "awsData = \"s3://amazon-reviews-pds/tsv/amazon_reviews_us_Music_v1_00.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: string (nullable = true)\n",
      " |-- helpful_votes: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import Word2VecModel\n",
    "\"\"\"Loading data into a dataframe\"\"\"\n",
    "musicData = spark.read.csv(awsData,header=True,sep='\\t')\n",
    "musicData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4751577"
     ]
    }
   ],
   "source": [
    "\"\"\"Selecting required columns\"\"\"\n",
    "requiredData = musicData.select('customer_id','product_id','product_title','star_rating','review_id','review_body')\n",
    "requiredData.cache()\n",
    "requiredData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Identifying top 10 products based on number of reviews received.\"\"\"\n",
    "\"\"\"Picking 10th product from the top 10 products identified\"\"\"\n",
    "top_10_product_ids=requiredData.filter(col(\"review_id\").isNotNull()).groupBy(\"product_id\").count().sort(col(\"count\").desc()).limit(10)\n",
    "top_10_product_ids=top_10_product_ids.sort(col(\"count\")).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Separating positive and negative reviews based on star rating\"\"\"\n",
    "positive_reviews=requiredData.join(top_10_product_ids.select(\"product_id\"),\"product_id\",\"inner\").filter(col(\"star_rating\")>=4)\n",
    "negative_reviews=requiredData.join(top_10_product_ids.select(\"product_id\"),\"product_id\",\"inner\").filter(col(\"star_rating\")<=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323"
     ]
    }
   ],
   "source": [
    "positive_reviews.cache()\n",
    "positive_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415"
     ]
    }
   ],
   "source": [
    "negative_reviews.cache()\n",
    "negative_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting review body into sentences based on . or ? and saving as list\"\"\"\n",
    "positive_review_sentences=positive_reviews.filter(col(\"review_body\").isNotNull()).select(\"review_id\",\"review_body\")\n",
    "positive_review_sentences=positive_review_sentences.withColumn(\"review_body_sentences\",split(col(\"review_body\"), r\"\\.|\\?\"))\n",
    "negative_review_sentences=negative_reviews.filter(col(\"review_body\").isNotNull()).select(\"review_id\",\"review_body\")\n",
    "negative_review_sentences=negative_review_sentences.withColumn(\"review_body_sentences\",split(col(\"review_body\"), r\"\\.|\\?\"))\n",
    "#positive_review_sentences.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Exploding review sentences to have one to many mapping for review_id vs review sentences. Also, cleaning review sentences \n",
    "of any special characters/trim additional spaces. Later on, we split sentence into words to remove any stop words in next step\"\"\"\n",
    "positive_review_sentences=positive_review_sentences.withColumn(\"review_body_sentences\",explode(\"review_body_sentences\"))\n",
    "positive_review_sentences=positive_review_sentences.withColumn(\"review_body_sentences_joined\",col(\"review_body_sentences\"))\n",
    "positive_review_sentences=positive_review_sentences.withColumn(\"review_body_sentences\",regexp_replace(\"review_body_sentences\",\"<br />|\\s+|$|,|!|#|@|<|>|/|&|#|;|:|[0-9]|-\",\" \")) \\\n",
    ".withColumn(\"review_body_sentences\",regexp_replace(\"review_body_sentences\",\"\\s+\",\" \")) \\\n",
    ".withColumn(\"review_body_sentences\",trim(col(\"review_body_sentences\"))) \\\n",
    ".filter(col(\"review_body_sentences\").isNotNull()) \\\n",
    ".filter(col(\"review_body_sentences\")!='') \\\n",
    ".withColumn(\"review_body_sentences\",lower(col(\"review_body_sentences\"))) \\\n",
    ".withColumn(\"review_body_sentences\",split(trim(col(\"review_body_sentences\")), \"\\s+\"))\n",
    "#positive_review_sentences.show(2,truncate=False)\n",
    "negative_review_sentences=negative_review_sentences.withColumn(\"review_body_sentences\",explode(\"review_body_sentences\"))\n",
    "negative_review_sentences=negative_review_sentences.withColumn(\"review_body_sentences_joined\",col(\"review_body_sentences\"))\n",
    "negative_review_sentences=negative_review_sentences.withColumn(\"review_body_sentences\",regexp_replace(\"review_body_sentences\",\"<br />|\\s+|$|,|!|#|@|<|>|/|&|#|;|:|[0-9]|-\",\" \")) \\\n",
    ".withColumn(\"review_body_sentences\",regexp_replace(\"review_body_sentences\",\"\\s+\",\" \")) \\\n",
    ".withColumn(\"review_body_sentences\",trim(col(\"review_body_sentences\"))) \\\n",
    ".filter(col(\"review_body_sentences\").isNotNull()) \\\n",
    ".filter(col(\"review_body_sentences\")!='') \\\n",
    ".withColumn(\"review_body_sentences\",lower(col(\"review_body_sentences\"))) \\\n",
    ".withColumn(\"review_body_sentences\",split(trim(col(\"review_body_sentences\")), \"\\s+\"))\n",
    "#positive_review_sentences.show(2,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stop words remover will remove stop words. Stop words list is retreived from nltk using below:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " set(stopwords.words('english'))\n",
    "\"\"\"\n",
    "stopwords_list=['a', 'ah','br',  'about', 'quot', 'above',  'after',  'again',  'against',  'ain',  'all',  'am',  'an',  'and',  'any',  'are',  'aren',  \"aren't\",  'as',  'at',  'be',  'because',  'been',  'before',  'being',  'below',  'between',  'both',  'but',  'by',  'can',  'couldn',  \"couldn't\",  'd',  'did',  'didn',  \"didn't\",  'do',  'does',  'doesn',  \"doesn't\",  'doing',  'don',  \"don't\",  'down',  'during',  'each',  'few',  'for',  'from',  'further',  'had',  'hadn',  \"hadn't\",  'has',  'hasn',  \"hasn't\",  'have',  'haven',  \"haven't\",  'having',  'he',  'her',  'here',  'hers',  'herself',  'him',  'himself',  'his',  'how',  'i',  'if',  'in',  'into',  'is',  'isn',  \"isn't\",  'it',  \"it's\",  'its',  'itself',  'just',  'll',  'm',  'ma',  'me',  'mightn',  \"mightn't\",  'more',  'most',  'mustn',  \"mustn't\",  'my',  'myself',  'needn',  \"needn't\",  'no',  'nor',  'not',  'now',  'o',  'of',  'off',  'on',  'once',  'only',  'or',  'other',  'our',  'ours',  'ourselves',  'out',  'over',  'own',  're',  's',  'same',  'shan',  \"shan't\",  'she',  \"she's\",  'should',  \"should've\",  'shouldn',  \"shouldn't\",  'so',  'some',  'such',  't',  'than',  'that',  \"that'll\",  'the',  'their',  'theirs',  'them',  'themselves',  'then',  'there',  'these',  'thanks',  'they',  'this',  'those',  'through',  'to',  'too',  'under',  'until',  'up',  've',  'very',  'was',  'wasn',  \"wasn't\",  'we',  'were',  'weren',  \"weren't\",  'what',  'when',  'where',  'which',  'while',  'who',  'whom',  'why',  'will',  'with',  'won',  \"won't\",  'wouldn',  \"wouldn't\",  'y',  'you',  \"you'd\",  \"you'll\",  \"you're\",  \"you've\",  'your',  'yours',  'yourself',  'yourselves']\n",
    "removerSW=StopWordsRemover(inputCol=\"review_body_sentences\",outputCol=\"filtered\", stopWords=stopwords_list)\n",
    "positive_review_sentences_new=removerSW.transform(positive_review_sentences)\n",
    "negative_review_sentences_new=removerSW.transform(negative_review_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8667"
     ]
    }
   ],
   "source": [
    "\"\"\"removing any sentences with less than two words in them\"\"\"\n",
    "positive_review_sentences_new=positive_review_sentences_new.filter(size(col(\"filtered\"))>2)\n",
    "positive_review_sentences_new.cache()\n",
    "positive_review_sentences_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3048"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import concat, length\n",
    "negative_review_sentences_new=negative_review_sentences_new.filter(size(col(\"filtered\"))>2)\n",
    "negative_review_sentences_new.cache()\n",
    "negative_review_sentences_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setting word2vec model with vector size of 100 and iterations 100. We fit the model first and then transform the sentences\n",
    "to vectors by calling transform function on dataframe\"\"\"\n",
    "vsize=100\n",
    "word2Vec_model = Word2Vec(vectorSize=vsize, windowSize=2,minCount=0, inputCol=\"filtered\", outputCol=\"features\",seed=42, maxIter=100, stepSize=0.1)\n",
    "model = word2Vec_model.fit(positive_review_sentences_new)\n",
    "result = model.transform(positive_review_sentences_new)\n",
    "model_n = word2Vec_model.fit(negative_review_sentences_new)\n",
    "result_n = model_n.transform(negative_review_sentences_new)\n",
    "#result.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementing PCA to convert 1x100 dimension vectors into 1x2 dimension vectors by picking 2 principal components.\"\"\"\n",
    "\"\"\"Setting up normaliser to allow taking dot product for cosine similarity directly\"\"\"\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(result)\n",
    "pca_result = model.transform(result).select(\"review_id\",\"pca_features\",\"review_body_sentences_joined\",\"filtered\")\n",
    "normalizer = Normalizer(inputCol=\"pca_features\", outputCol=\"features_norm\", p=2.0)\n",
    "pca_result = normalizer.transform(pca_result)\n",
    "#pca_result.show(2)\n",
    "pca_n = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model_n = pca_n.fit(result_n)\n",
    "pca_result_n = model_n.transform(result_n).select(\"review_id\",\"pca_features\",\"review_body_sentences_joined\",\"filtered\")\n",
    "normalizer_n = Normalizer(inputCol=\"pca_features\", outputCol=\"features_norm\", p=2.0)\n",
    "pca_result_n = normalizer_n.transform(pca_result_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 54810)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "----------------------------------------"
     ]
    }
   ],
   "source": [
    "\"\"\"Appending row number with index column name.\"\"\"\n",
    "pca_result_withIndex = pca_result.rdd.zipWithUniqueId().map(lambda row: (row[1],row[0][0],row[0][1],row[0][2],row[0][3],row[0][4])).toDF()\n",
    "pca_result_withIndex = pca_result_withIndex.select(col(\"_1\").alias(\"index\"), col(\"_2\").alias(\"review_id\"), col(\"_3\").alias(\"pca_features\"), col(\"_4\").alias(\"review_body_sentences_joined\"), col(\"_5\").alias(\"filtered\"), col(\"_6\").alias(\"features_norm\"))\n",
    "#pca_result_withIndex.show(2)\n",
    "pca_result_withIndex_n = pca_result_n.rdd.zipWithUniqueId().map(lambda row: (row[1],row[0][0],row[0][1],row[0][2],row[0][3],row[0][4])).toDF()\n",
    "pca_result_withIndex_n = pca_result_withIndex_n.select(col(\"_1\").alias(\"index\"), col(\"_2\").alias(\"review_id\"), col(\"_3\").alias(\"pca_features\"), col(\"_4\").alias(\"review_body_sentences_joined\"), col(\"_5\").alias(\"filtered\"), col(\"_6\").alias(\"features_norm\"))\n",
    "#pca_result_withIndex_n.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collecting all positive normalised vectors into numpy array and reshaping to have each row to contain \n",
    "vector from corresponding review sentence \"\"\"\n",
    "all_vectors_array=np.array(pca_result_withIndex.select(\"features_norm\").collect())\n",
    "all_vectors_array=all_vectors_array.reshape((all_vectors_array.shape[0],2))\n",
    "#all_vectors_array.shape\n",
    "\"\"\"Similarly for negative\"\"\"\n",
    "all_vectors_array_n=np.array(pca_result_withIndex_n.select(\"features_norm\").collect())\n",
    "all_vectors_array_n=all_vectors_array_n.reshape((all_vectors_array_n.shape[0],2))\n",
    "#all_vectors_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defining function to take intra class similarity. It takes dot product between normalised features with the array formed \n",
    "in previous steps. We exclude the vector itself from the main array as we aim to find similarity between other vectors. This \n",
    "is achieved by droping the column at index of the row feature as took transpose for dot product.Once dot product is done, \n",
    "we take 1-dot product to get cosine distance and take average using numpy.mean\"\"\"\n",
    "\"\"\"Applying UDF defined for both negative and positive cases\"\"\"\n",
    "def intra_class_similarity(features_vec,index):\n",
    "    current_vec=np.array(features_vec)[np.newaxis,:]\n",
    "    all_vec_norm=all_vectors_array.T\n",
    "    all_vec_norm=np.delete(all_vec_norm, index, axis=1)\n",
    "    avg_dist=np.mean(1-np.dot(current_vec,all_vec_norm))\n",
    "    return float(avg_dist)\n",
    "\n",
    "def intra_class_similarity_n(features_vec,index):\n",
    "    current_vec=np.array(features_vec)[np.newaxis,:]\n",
    "    all_vec_norm=all_vectors_array_n.T\n",
    "    all_vec_norm=np.delete(all_vec_norm, index, axis=1)\n",
    "    avg_dist=np.mean(1-np.dot(current_vec,all_vec_norm))\n",
    "    return float(avg_dist)\n",
    "\n",
    "intra_class_similarity_udf = udf(intra_class_similarity, FloatType())\n",
    "intra_class_similarity_n_udf = udf(intra_class_similarity_n, FloatType())\n",
    "pca_result_withIndex=pca_result_withIndex.withColumn(\"avg_distance\", intra_class_similarity_udf(pca_result_withIndex[\"features_norm\"],pca_result_withIndex[\"index\"]))\n",
    "pca_result_withIndex_n=pca_result_withIndex_n.withColumn(\"avg_distance\", intra_class_similarity_n_udf(pca_result_withIndex_n[\"features_norm\"],pca_result_withIndex_n[\"index\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_upd=pca_result_withIndex.select(\"index\",\"review_id\",\"review_body_sentences_joined\",\"filtered\",\"features_norm\",\"avg_distance\").sort(col(\"avg_distance\"))\n",
    "pca_result_upd_n=pca_result_withIndex_n.select(\"index\",\"review_id\",\"review_body_sentences_joined\",\"filtered\",\"features_norm\",\"avg_distance\").sort(col(\"avg_distance\"))\n",
    "#pca_result_upd.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extracting case center by sorting in ascending based on average distance\"\"\"\n",
    "positive_center=pca_result_upd.select(\"index\",\"review_id\",\"features_norm\",\"avg_distance\",col(\"review_body_sentences_joined\").alias(\"center_sentence_text\")).limit(1)\n",
    "negative_center=pca_result_upd_n.select(\"index\",\"review_id\",\"features_norm\",\"avg_distance\",col(\"review_body_sentences_joined\").alias(\"center_sentence_text\")).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Maximum value of avg_distance for positive is \",pca_result_upd.agg({\"avg_distance\": \"max\"}).collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Using center identified, we apply the function similar to previous one, to identify distance and sort in ascending order. \n",
    "The first element is considered to be iself as vector will have 0 distance(least) with itself. \"\"\"\n",
    "\n",
    "def ten_most_similar(features_vec,index):\n",
    "    current_vec=np.array(features_vec)[np.newaxis,:]\n",
    "    all_vec_norm=all_vectors_array.T\n",
    "    #all_vec_norm=np.delete(all_vec_norm, index, axis=1)\n",
    "    index_list=np.argsort(1-np.dot(current_vec,all_vec_norm))[:,1:11][0]\n",
    "    return index_list.tolist()\n",
    "\n",
    "def ten_most_similar_n(features_vec,index):\n",
    "    current_vec=np.array(features_vec)[np.newaxis,:]\n",
    "    all_vec_norm=all_vectors_array_n.T\n",
    "    #all_vec_norm=np.delete(all_vec_norm, index, axis=1)\n",
    "    index_list=np.argsort(1-np.dot(current_vec,all_vec_norm))[:,1:11][0]\n",
    "    return index_list.tolist()\n",
    "\n",
    "ten_most_similar_udf = udf(ten_most_similar, ArrayType(IntegerType()))\n",
    "ten_most_similar_n_udf = udf(ten_most_similar_n, ArrayType(IntegerType()))\n",
    "ten_most_similar_df=positive_center.withColumn(\"10_most_similar_index\",ten_most_similar_udf(col(\"features_norm\"),col(\"index\")))\n",
    "ten_most_similar_df=ten_most_similar_df.drop(col(\"index\")).withColumn(\"index\",explode(\"10_most_similar_index\"))\n",
    "ten_most_similar_n_df=negative_center.withColumn(\"10_most_similar_index\",ten_most_similar_n_udf(col(\"features_norm\"),col(\"index\")))\n",
    "ten_most_similar_n_df=ten_most_similar_n_df.drop(col(\"index\")).withColumn(\"index\",explode(\"10_most_similar_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Based on index identified in previous step, we make a join with main dataframe and extract results in required format.\"\"\"\n",
    "final_result=ten_most_similar_df.select(col(\"review_id\").alias(\"center_review_id\"),col(\"avg_distance\").alias(\"center_avg_dist\"),col(\"index\"), col(\"center_sentence_text\")).join(pca_result_upd,\"index\", \"inner\")\n",
    "final_result_n=ten_most_similar_n_df.select(col(\"review_id\").alias(\"center_review_id\"),col(\"avg_distance\").alias(\"center_avg_dist\"),col(\"index\"), col(\"center_sentence_text\")).join(pca_result_upd_n,\"index\", \"inner\")\n",
    "#final_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_final=final_result.select(\"center_review_id\",\"center_sentence_text\",col(\"review_id\").alias(\"10_nearestneighbour_rv_id\"), col(\"review_body_sentences_joined\").alias(\"corresponding_sentence\"))\n",
    "negative_final=final_result_n.select(\"center_review_id\",\"center_sentence_text\",col(\"review_id\").alias(\"10_nearestneighbour_rv_id\"), col(\"review_body_sentences_joined\").alias(\"corresponding_sentence\"))\n",
    "#df.write.parquet(\"s3a://bucket-name/shri/test.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-------------------------+----------------------+\n",
      "|center_review_id|center_sentence_text|10_nearestneighbour_rv_id|corresponding_sentence|\n",
      "+----------------+--------------------+-------------------------+----------------------+\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|            RTEW7ZYKXX2TB|   I mean at least ...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|           R191KI5DAR1DNF|   Just because bec...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|           R1X96BRSNMO8OU|   And i hope that ...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|           R3BT24PSNC8E7J|    Inside there is...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|            R962EPIVN1H8J|   They seem like t...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|           R2F2ESD4D4VHRD|  I usually dislike...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|           R3PPZZTVMCTOGD|    not all music c...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|           R3EJV8X5FA1MUC|    Still Joel, Ben...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|           R3TJUAOQE3UG0T|    This, cd opitim...|\n",
      "|  R380QK1PRRA9V5|\\\\\" It's a song w...|            RRIM8Z5B6V2B7|   People have writ...|\n",
      "+----------------+--------------------+-------------------------+----------------------+"
     ]
    }
   ],
   "source": [
    "\"\"\"Result for positive case. It includes center review id, center sentence text, \n",
    "10 nearest neighbours review id and their corresponding sentences\"\"\"\n",
    "positive_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-------------------------+----------------------+\n",
      "|center_review_id|center_sentence_text|10_nearestneighbour_rv_id|corresponding_sentence|\n",
      "+----------------+--------------------+-------------------------+----------------------+\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|           R1JV7HP59NBLEF|   it must really s...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|            R2C39UD7SM4S7|    It's a [weak] e...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|            R1H0G8SIFXMTV|   With this new cd...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|            RPRKZI8RFOP4Q|   White Strpes are...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|           R1RC2DNAX46F37|  What ever happene...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|           R2ENY6FUCP8901|  Well looking at t...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|           R1J1X1BIVUG431|   Just keep tellin...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|           R1SFOZZMB69VDS|   Just keep tellin...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|           R1J1X1BIVUG431|  u guyz who say GC...|\n",
      "|  R24EC70OSKLM9S|Yes! Another anti...|           R1SFOZZMB69VDS|  u guyz who say GC...|\n",
      "+----------------+--------------------+-------------------------+----------------------+"
     ]
    }
   ],
   "source": [
    "\"\"\"Result for negative case. It includes center review id, center sentence text, \n",
    "10 nearest neighbours review id and their corresponding sentences\"\"\"\n",
    "negative_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|index|review_id     |features_norm                           |avg_distance|center_sentence_text                                                                                                                                     |\n",
      "+-----+--------------+----------------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|7728 |R380QK1PRRA9V5|[0.39348735810627605,0.9193300272538384]|0.9360323   |\\\\\" It's a song where you're feeling depressed and no one cares and they are saying Hold On things will get better and we all have [things] to go through|\n",
      "+-----+--------------+----------------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "positive_center.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+------------+--------------------+\n",
      "|index|     review_id|       features_norm|avg_distance|center_sentence_text|\n",
      "+-----+--------------+--------------------+------------+--------------------+\n",
      "|  313|R24EC70OSKLM9S|[0.16685070289897...|   0.8396373|Yes! Another anti...|\n",
      "+-----+--------------+--------------------+------------+--------------------+"
     ]
    }
   ],
   "source": [
    "negative_center.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0:06:05.498508"
     ]
    }
   ],
   "source": [
    "end_time=datetime.datetime.now()\n",
    "print(\"Execution time:\",(end_time-start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
